{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# sklearn\n",
    "This is a machine learning library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "## Setup (hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "hide_input": false,
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; margin-left:350px; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; margin-left:350px; }</style>\"))\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "pd.set_option( 'display.notebook_repr_html', False)  # render Series and DataFrame as text, not HTML\n",
    "pd.set_option( 'display.max_column', 10)    # number of columns\n",
    "pd.set_option( 'display.max_rows', 10)     # number of rows\n",
    "pd.set_option( 'display.width', 90)        # number of characters per row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## The Library\n",
    "sklearn **does not automatically import its subpackages**. Therefore all subpakcages must be specifically loaded before use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing   import Imputer\n",
    "from sklearn.preprocessing   import MinMaxScaler\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.preprocessing   import Normalizer\n",
    "from sklearn.preprocessing   import PolynomialFeatures\n",
    "\n",
    "# Model and Pipeline\n",
    "from sklearn.linear_model    import LinearRegression\n",
    "from sklearn.pipeline        import make_pipeline\n",
    "\n",
    "# Measurement\n",
    "from sklearn.metrics         import *\n",
    "\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![split](img/fitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting\n",
    "- The model does not fit the training data and therefore misses the trends in the data\n",
    "- The model cannot be generalized to new data, this is usually the result of a very simple model (not enough predictors/independent variables)\n",
    "- The model will have poor predictive ability\n",
    "- For example, we fit a linear model (like linear regression) to data that is not linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "- The model has trained “too well” and is now, well, fit too closely to the training dataset\n",
    "- The model is too complex (i.e. **too many features/variables** compared to the number of observations)\n",
    "- The model will be very accurate on the training data but will probably be very not accurate on untrained or new data\n",
    "- The model is not generalized (or not AS generalized), meaning you can generalize the results \n",
    "- The model learns or describes the “noise” in the training data instead of the actual relationships between variables in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just Right\n",
    "- It is worth noting the underfitting is not as prevalent as overfitting\n",
    "- Nevertheless, we want to avoid both of those problems in data analysis\n",
    "- We want to find the middle ground between under and overfitting our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning\n",
    "- A highly complex model tend to overfit\n",
    "- A too flexible model tend to underfit\n",
    "\n",
    "Complexity can be reduced by: \n",
    "- Less features\n",
    "- Less degree of polynomial features\n",
    "- Apply generalization (tuning hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![split](img/model_complexity.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![split](img/scikit_learn_split.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data\n",
    "Generate 100 rows of data, with 3x features (X1,X2,X3), and one dependant variable (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 4)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 21  # number of samples\n",
    "I = 5  # intercept value\n",
    "E = np.random.randint( 1,20, n)  # Error\n",
    "x1 = np.random.randint( 1,n+1, n)\n",
    "x2 = np.random.randint( 1,n+1, n)\n",
    "x3 = np.random.randint( 1,n+1, n)\n",
    "y = 0.1*x1 + 0.2*x2 + 0.3*x3 + E + I\n",
    "mydf = pd.DataFrame({\n",
    "    'y':y,\n",
    "    'x1':x1,\n",
    "    'x2':x2,\n",
    "    'x3':x3\n",
    "})\n",
    "mydf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Time Split\n",
    "\n",
    "sklearn::train_test_split() has two forms:\n",
    "- Take one DF, split into 2 DF (most of sklearn modeling use this method\n",
    "- Take two DFs, split into 4 DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   x1  x2  x3     y\n",
       "0  10  17  12  21.0\n",
       "1  12   5   9  16.9\n",
       "2  18  13   2  27.0\n",
       "3  11  19  17  22.0\n",
       "4  18  13  10  24.4"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Split One Dataframe Into Two (Train & Test)\n",
    "\n",
    "```\n",
    "traindf, testdf = train_test_split( df, test_size=, random_state= ) \n",
    " # random_state : seed number (integer), optional\n",
    " # test_size    : fraction of 1, 0.2 means 20%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf, testdf = train_test_split(mydf,test_size=0.2, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print (len(traindf))\n",
    "print (len(testdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Split Two DataFrame (X,Y) into Four x_train/test, y_train/test\n",
    "```\n",
    "x_train, x_test, y_train, y_test = train_test_split( X,Y, test_size=, random_state= )\n",
    " # random_state : seed number (integer), optional\n",
    " # test_size    : fraction of 1, 0.2 means 20%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split DataFrame into X and Y First**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['x1','x2','x3']\n",
    "X = mydf[feature_cols]\n",
    "Y = mydf.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then Split X/Y into x_train/test, y_train/test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split( X,Y, test_size=0.2, random_state=25)\n",
    "print (len(x_train))\n",
    "print (len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold\n",
    "```\n",
    "KFold(n_splits=3, shuffle=False, random_state=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**suffle=False** (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20] [0 1 2]\n",
      "[ 0  1  2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20] [3 4 5]\n",
      "[ 0  1  2  3  4  5  9 10 11 12 13 14 15 16 17 18 19 20] [6 7 8]\n",
      "[ 0  1  2  3  4  5  6  7  8 12 13 14 15 16 17 18 19 20] [ 9 10 11]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 15 16 17 18 19 20] [12 13 14]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 18 19 20] [15 16 17]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17] [18 19 20]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "  print (train_index, test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**shuffle=True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20] [1 3 5]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 15 17 18 19 20] [13 14 16]\n",
      "[ 0  1  2  3  4  5  6  9 10 11 12 13 14 15 16 17 19 20] [ 7  8 18]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 16 17 18 20] [11 15 19]\n",
      "[ 1  2  3  4  5  6  7  8  9 11 13 14 15 16 17 18 19 20] [ 0 10 12]\n",
      "[ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 18 19] [ 4 17 20]\n",
      "[ 0  1  3  4  5  7  8 10 11 12 13 14 15 16 17 18 19 20] [2 6 9]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "  print (train_index, test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave One Out\n",
    "\n",
    "- For a dataset of N rows, Leave One Out will split N-1 times, each time leaving one row as test, remaning as training set.  \n",
    "- Due to the **high number of test sets** (which is the same as the number of samples-1) this cross-validation method can be very costly. For large datasets one should favor KFold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = LeaveOneOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20] [0]\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20] [1]\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20] [2]\n",
      "[ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20] [3]\n",
      "[ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20] [4]\n",
      "[ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20] [5]\n",
      "[ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20] [6]\n",
      "[ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20] [7]\n",
      "[ 0  1  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20] [8]\n",
      "[ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20] [9]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 20] [10]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 17 18 19 20] [11]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20] [12]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 20] [13]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18 19 20] [14]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 17 18 19 20] [15]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20] [16]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20] [17]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 19 20] [18]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20] [19]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19] [20]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in loo.split(X):\n",
    "  print (train_index, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    x1  x2  x3\n",
       "0   10  17  12\n",
       "1   12   5   9\n",
       "2   18  13   2\n",
       "3   11  19  17\n",
       "4   18  13  10\n",
       "..  ..  ..  ..\n",
       "16  20  13  18\n",
       "17   1  17   1\n",
       "18  13  20  18\n",
       "19  11   4  15\n",
       "20   6   2  10\n",
       "\n",
       "[21 rows x 3 columns]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Transform\n",
    "This can be used as part of feature engineering, to introduce new features for data that seems to fit with quadradic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Data\n",
    "Data must be 2-D before polynomial features can be applied. Code below convert 1D array into 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5]])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3, 4, 5])\n",
    "X = x[:,np.newaxis]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree 1\n",
    "One Degree means maintain original features. No new features is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 4.],\n",
       "       [ 5.]])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PolynomialFeatures(degree=1, include_bias=False).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree 2\n",
    "Degree-1 original   feature:  x  \n",
    "Degree-2 additional features:  x^2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,   1.],\n",
       "       [  2.,   4.],\n",
       "       [  3.,   9.],\n",
       "       [  4.,  16.],\n",
       "       [  5.,  25.]])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PolynomialFeatures(degree=2, include_bias=False).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree 3\n",
    "Degree-1 original   feature:  x  \n",
    "Degree-2 additional features:  x^2  \n",
    "Degree-3 additional features:  x^3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.,    1.,    1.],\n",
       "       [   2.,    4.,    8.],\n",
       "       [   3.,    9.,   27.],\n",
       "       [   4.,   16.,   64.],\n",
       "       [   5.,   25.,  125.]])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PolynomialFeatures(degree=3, include_bias=False).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree 4\n",
    "Degree-1 original   feature:  x  \n",
    "Degree-2 additional features:  x^2  \n",
    "Degree-3 additional features:  x^3  \n",
    "Degree-3 additional features:  x^4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.,    1.,    1.,    1.],\n",
       "       [   2.,    4.,    8.,   16.],\n",
       "       [   3.,    9.,   27.,   81.],\n",
       "       [   4.,   16.,   64.,  256.],\n",
       "       [   5.,   25.,  125.,  625.]])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PolynomialFeatures(degree=4, include_bias=False).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   x1  x2\n",
       "0   1   6\n",
       "1   2   7\n",
       "2   3   8\n",
       "3   4   9\n",
       "4   5  10"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame( {'x1': [1, 2, 3, 4, 5 ],\n",
    "                   'x2': [6, 7, 8, 9, 10]})\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree 2\n",
    "```\n",
    "Degree-1 original   features:  x1,     x2  \n",
    "Degree-2 additional features:  x1^2,   x2^2,   x1:x2 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.,    6.,    1.,    6.,   36.],\n",
       "       [   2.,    7.,    4.,   14.,   49.],\n",
       "       [   3.,    8.,    9.,   24.,   64.],\n",
       "       [   4.,    9.,   16.,   36.,   81.],\n",
       "       [   5.,   10.,   25.,   50.,  100.]])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PolynomialFeatures(degree=2, include_bias=False).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree 3\n",
    "```\n",
    "Degree-1 original   features:  x1,       x2  \n",
    "Degree-2 additional features:  x1^2,     x2^2,   x1:x2 \n",
    "Degree-3 additional features:  x1^3,     x2^3    x1:x2^2    x2:x1^2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1.,     6.,     1.,     6.,    36.,     1.,     6.,    36.,\n",
       "          216.],\n",
       "       [    2.,     7.,     4.,    14.,    49.,     8.,    28.,    98.,\n",
       "          343.],\n",
       "       [    3.,     8.,     9.,    24.,    64.,    27.,    72.,   192.,\n",
       "          512.],\n",
       "       [    4.,     9.,    16.,    36.,    81.,    64.,   144.,   324.,\n",
       "          729.],\n",
       "       [    5.,    10.,    25.,    50.,   100.,   125.,   250.,   500.,\n",
       "         1000.]])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PolynomialFeatures(degree=3, include_bias=False).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation of Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "X = np.array([[ nan, 0,   3  ],\n",
    "              [ 3,   7,   9  ],\n",
    "              [ 3,   5,   2  ],\n",
    "              [ 4,   nan, 6  ],\n",
    "              [ 8,   8,   1  ]])\n",
    "\n",
    "y = np.array([14, 16, -1,  8, -5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.5,  0. ,  3. ],\n",
       "       [ 3. ,  7. ,  9. ],\n",
       "       [ 3. ,  5. ,  2. ],\n",
       "       [ 4. ,  5. ,  6. ],\n",
       "       [ 8. ,  8. ,  1. ]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp = Imputer(strategy='mean')\n",
    "X2 = imp.fit_transform(X)\n",
    "X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "It is possible that some insignificant variable with larger range will be dominating the objective function.  \n",
    "We can remove this problem by scaling down all the features to a same range.\n",
    "\n",
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   x1  x2  x3\n",
       "0  10  17  12\n",
       "1  12   5   9\n",
       "2  18  13   2\n",
       "3  11  19  17\n",
       "4  18  13  10"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=mydf.filter(like='x')[:5]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax Scaler\n",
    "\n",
    "```\n",
    "MinMaxScaler( feature_range(0,1), copy=True )\n",
    "# default feature range (output result) from 0 to 1\n",
    "# default return a copy of new array, copy=False will inplace original array\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Scaler Object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.85714286,  0.66666667],\n",
       "       [ 0.25      ,  0.        ,  0.46666667],\n",
       "       [ 1.        ,  0.57142857,  0.        ],\n",
       "       [ 0.125     ,  1.        ,  1.        ],\n",
       "       [ 1.        ,  0.57142857,  0.53333333]])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaler Attributes**  \n",
    "```\n",
    "data_min_: minimum value of the feature (before scaling)  \n",
    "data_max_: maximum value of the feature (before scaling)  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    data_min  data_max\n",
       "x1      10.0      18.0\n",
       "x2       5.0      19.0\n",
       "x3       2.0      17.0"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(zip(scaler.data_min_, scaler.data_max_)), \n",
    "             columns=['data_min','data_max'], \n",
    "             index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler\n",
    "\n",
    "It is most suitable for techniques that assume a Gaussian distribution in the input variables and work better with rescaled data, such as linear regression, logistic regression and linear discriminate analysis.\n",
    "\n",
    "```\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "# copy=True : return a copy of data, instead of inplace\n",
    "# with_mean=True : centre all features by substracting with its mean\n",
    "# with_std=True  : centre all features by dividing with its std\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Scaler Object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.08972474,  0.75      ,  0.41169348],\n",
       "       [-0.5161854 , -1.75      , -0.20584674],\n",
       "       [ 1.2044326 , -0.08333333, -1.64677394],\n",
       "       [-0.80295507,  1.16666667,  1.4409272 ],\n",
       "       [ 1.2044326 , -0.08333333,  0.        ]])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaler Attributes**  \n",
    "After the data transformation step above, scaler will have the mean and variance information for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    mean  variance\n",
       "x1  13.8     12.16\n",
       "x2  13.4     23.04\n",
       "x3  10.0     23.60"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(zip(scaler.mean_, scaler.var_)), \n",
    "             columns=['mean','variance'], \n",
    "             index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With any of the preceding examples, it can quickly become tedious to do the transformations by hand, especially if you wish to string together multiple steps. For example, we might want a processing pipeline that looks something like this:\n",
    "\n",
    "- **Impute** missing values using the mean  \n",
    "- **Transform** features to quadratic  \n",
    "- **Fit** a linear regression  \n",
    "\n",
    "**make_pipeline** takes list of functions as parameters. When calling **fit()** on a pipeline object, these functions will be performed in sequential with data flow from one function to another.\n",
    "\n",
    "```\n",
    "make_pipeline (\n",
    "    function_1 (),\n",
    "    function_2 (),\n",
    "    function_3 ()\n",
    " )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   x1  x2  x3\n",
       "0  10  17  12\n",
       "1  12   5   9\n",
       "2  18  13   2\n",
       "3  11  19  17\n",
       "4  18  13  10"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 16, -1,  8, -5])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.pipeline.Pipeline"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pipe = make_pipeline (\n",
    "    Imputer            (strategy='mean'),\n",
    "    PolynomialFeatures (degree=2),\n",
    "    LinearRegression   ()\n",
    ")\n",
    "type(my_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('linearregression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14 16 -1  8 -5]\n",
      "[ 14.  16.  -1.   8.  -5.]\n"
     ]
    }
   ],
   "source": [
    "my_pipe.fit( X, y) # execute the pipeline\n",
    "print (y)\n",
    "print (my_pipe.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.pipeline.Pipeline"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(my_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11],\n",
       "       [12, 13, 14],\n",
       "       [15, 16, 17],\n",
       "       [18, 19, 20]])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(21).reshape(7,3)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold\n",
    "```\n",
    "KFold(n_splits=3, shuffle=False, random_state=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=7, random_state=None, shuffle=False)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6] [0]\n",
      "[0 2 3 4 5 6] [1]\n",
      "[0 1 3 4 5 6] [2]\n",
      "[0 1 2 4 5 6] [3]\n",
      "[0 1 2 3 5 6] [4]\n",
      "[0 1 2 3 4 6] [5]\n",
      "[0 1 2 3 4 5] [6]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    " print (train_index, test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave One Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "868px",
    "left": "0px",
    "right": "1043.45px",
    "top": "110px",
    "width": "353px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "773px",
    "left": "1252.31px",
    "right": "20px",
    "top": "201.979px",
    "width": "709px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
